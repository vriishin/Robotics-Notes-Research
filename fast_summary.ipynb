{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eae3f6c",
   "metadata": {},
   "source": [
    "# FAST: Efficient Action Tokenization for Vision-Language-Action Models\n",
    "\n",
    "**Reference:** Pertsch et al., arXiv:2501.09747v1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe5568",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "FAST introduces a compression-based tokenization method using the Discrete Cosine Transform (DCT) to compress continuous robot action sequences before tokenizing. This approach reduces correlation between tokens and enables effective autoregressive VLA training on high-frequency, dexterous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841769d4",
   "metadata": {},
   "source": [
    "## Key Contributions\n",
    "\n",
    "- Identified limitations of per-dimension binning for high-frequency control.\n",
    "- Proposed FAST: DCT-based compression for action tokenization.\n",
    "- Released FAST+, a universal tokenizer trained on 1M trajectories.\n",
    "- Demonstrated 5× faster training on 10k hours of data, matching diffusion VLA performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2d9e5",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "1. **Normalize** action dimensions to [–1,1] range.\n",
    "2. **Apply DCT** to each action chunk.\n",
    "3. **Quantize** top-k coefficients and encode via byte-pair encoding.\n",
    "4. **Train** autoregressive Transformer on compressed tokens.\n",
    "\n",
    "![FAST pipeline](https://pi.website/research/fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8b978",
   "metadata": {},
   "source": [
    "## Experiments & Results\n",
    "\n",
    "- Toy interpolation tasks: FAST maintains low MSE across sampling rates, unlike naive binning.\n",
    "- DROID dataset: first autoregressive VLA on high-frequency data.\n",
    "- 10k-hour robot datasets: FAST-π0 matches diffusion VLA performance, 5× faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7f9fb",
   "metadata": {},
   "source": [
    "## Connections to Other Papers\n",
    "\n",
    "This work underpins later VLA developments:  \n",
    "- Forms the tokenization backbone in π0.5’s pretraining.  \n",
    "- Inspires compression steps in real-time chunking inference.  \n",
    "\n",
    "**Key Related Works:**\n",
    "- Byte Pair Encoding for NLP [27]\n",
    "- DROID dataset paper [38]\n",
    "- Diffusion VLA methods [7]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
