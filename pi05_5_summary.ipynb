{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360f3d06",
   "metadata": {},
   "source": [
    "# π0.5: A Vision-Language-Action Model with Open-World Generalization\n",
    "\n",
    "**Reference:** Black et al., pi05.pdf\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be474b6",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "π0.5 co-trains on heterogeneous data sources—including mobile and static robot datasets, high-level semantic subtasks, verbal instructions, and web vision-language tasks—to achieve robust open-world generalization for long-horizon manipulation in unseen homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d2681",
   "metadata": {},
   "source": [
    "## Key Contributions\n",
    "\n",
    "- Multi-source co-training on web data, semantic subtasks, and diverse robot demonstrations.\n",
    "- Two-stage training: broad pretraining with FAST tokens, then post-training with flow matching on mobile manipulation.\n",
    "- Demonstrated long-horizon tasks (10–15 min), e.g., kitchen cleaning, zero-shot in new homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ffd1e",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "1. **Pretraining Stage:** Use FAST tokenization on large, heterogeneous dataset (97.6% non-mobile robot/web data).\n",
    "2. **Post-training Stage:** Fine-tune on mobile manipulation with continuous action expert (flow matching) and verbal instructions.\n",
    "3. **Two-Level Inference:** First predict semantic subtask, then low-level action chunk.\n",
    "\n",
    "![π0.5 architecture](https://pi.website/blog/pi05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1fd761",
   "metadata": {},
   "source": [
    "## Experiments & Results\n",
    "\n",
    "- Zero-shot generalization in 20+ unseen homes.\n",
    "- Tasks include cleaning kitchens, bedrooms, making beds, hanging towels.\n",
    "- Outperforms π0 and other baselines on multi-stage household tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9ba1f",
   "metadata": {},
   "source": [
    "## Connections to Other Papers\n",
    "\n",
    "- **FAST:** Used for discrete token pretraining.  \n",
    "- **Knowledge Insulation:** Training recipe for discrete backbone vs. continuous expert.  \n",
    "- **Real-Time Chunking:** π0.5 provides the VLA base policy for RTC’s inference improvements.\n",
    "\n",
    "**Key Related Works:**\n",
    "- π0 VLA [7]\n",
    "- Multimodal VLA frameworks [23]\n",
    "- Semantic subtask prediction methods [44]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b61de7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Non-Technical Highlights\n",
    "### Definitions\n",
    "  “Broadening the training data distribution … allows the resulting policies to not only solve a wider range of tasks out of the box, but also improves their ability to generalize to new scenes and tasks.”\n",
    "  #\n",
    "    Data sources (in order of impact)\n",
    "    CE – Cross-embodiment robot data (static arms, dual arms, wheeled bases)\n",
    "    ME – Multi-environment static-arm data\n",
    "    WD – Web vision–language data\n",
    "    MM – mobile manipulator data (400 h from 100 real homes)\n",
    "    HL – Human-labelled sub-task sequences from tele-operation\n",
    "\n",
    "### Explanation\n",
    "- **What is π0.5?**  \n",
    "  A foundation model that lets a mobile robot do long chores—like picking clothes off a bedroom floor or putting dishes in the sink—in homes it’s never seen before.\n",
    "\n",
    "### Key Notes\n",
    "- **Heterogeneous Data is Crucial:**  \n",
    "  Training mixes robot demos, static-arm experiments, and billions of web image–text pairs to build broad “common sense.”\n",
    "- **Two-Stage Training (“Hybrid Scheme”):**  \n",
    "  1. **Big-picture stage:** Train on discrete tokens (like words) from cheap, varied data—static arms, web pictures, human scribbles.  \n",
    "  2. **Detail stage:** Fine-tune on real robot motions at 50 Hz for precise, responsive control.\n",
    "- **Examples & Metaphors:**  \n",
    "  - Think of stage 1 as **reading manuals** and watching **highlight reels**; stage 2 is **practice drills** in the field.\n",
    "  - Web data teaches the robot new “words” (like “peach”), even if never shown a real peach during robot demos.\n",
    "\n",
    "### Key Takeaways\n",
    "- **400 hours** of direct robot demos are enough when backed by diverse data sources.\n",
    "- Web-sourced captions and Q&A give robots a **vocabulary** for unseen objects.\n",
    "- High-level task labels help plan chores like a **to-do list**, but most benefits stick in the model’s “muscle memory” even if not explicitly used at runtime.\n",
    "\n",
    "### Limitations & Future Work\n",
    "- **Handle surprises:** Hard drawer handles or occluded spills can still stump the robot.\n",
    "- **Subtask loops:** Sometimes it reopens a drawer it just closed—like getting stuck in a maze.\n",
    "- **Next steps:** Scale up spoken corrections, on-the-fly demos, and richer context (like memory of prior rooms).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
